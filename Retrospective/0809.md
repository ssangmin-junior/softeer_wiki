## Keep

- **LLM 작업 진행**
    - LLM을 이용해 커뮤니티와 뉴스기사에서 어떤 결함에 대해 이야기하는지 분류하는 작업을 성공적으로 짰음.
    - 각 글을 요약하는 모델을 구축 중이며, 작업을 꾸준히 이어가고 있음.
- **데이터 크롤링**
    - 결함 이슈에 대한 내용을 데이터베이스(DB)로 구축하기 위해 크롤링 작업을 수행함.
    - 크롤링을 통해 결함 이슈와 관련된 데이터를 수집하고, 이를 DB에 적재할 수 있도록 준비함.
- **수업 참여 및 학습**
    - Stream vs Batch에 대한 수업을 통해 데이터를 실시간으로 처리할지 배치 방식으로 처리할지에 대한 개념을 학습함.
    - EDA(탐색적 데이터 분석) 후, 데이터가 어떻게 바뀌는지, 사용자가 원하는 정보의 빈도에 따라 실시간 데이터 처리(Kafka) 또는 배치 처리 방식을 선택하는 방법을 이해함.

## Problem

- **LLM 작업의 추가 과제**
    - 각 글을 요약하는 모델 구축 과정에서 새로운 과제와 도전에 직면함.
    - 요약 모델의 정확성과 효율성을 높이는 방법에 대해 추가적인 고민이 필요함.
- **크롤링 데이터의 처리**
    - 크롤링한 데이터를 효과적으로 DB에 구축하고, 이를 적절히 활용하기 위한 추가 작업이 필요함.
    - 수집된 데이터를 구조화하고, 분석 가능한 상태로 만드는 데 추가적인 노력이 요구됨.

## Try

- **요약 모델 구축 및 최적화**
    - LLM을 통해 분류된 데이터를 기반으로 각 글을 요약하는 모델을 지속적으로 구축하고 최적화하기.
    - 요약 모델의 성능을 테스트하고, 필요한 경우 데이터 전처리나 모델 튜닝을 통해 성능을 향상시키기.
- **크롤링 데이터의 처리 및 활용**
    - 크롤링한 데이터를 DB에 구조화하여 적재하고, 분석 가능한 상태로 만들기.
    - 데이터의 품질을 유지하기 위해 중복 데이터 제거, 데이터 정제 등의 작업을 수행.
    - 수집된 데이터를 기반으로 실시간 또는 배치 방식의 분석을 진행하여 결함 이슈에 대한 인사이트를 도출하기.
- **Stream vs Batch 결정**
    - 수업에서 배운 내용을 바탕으로, 실시간 데이터를 즉각적으로 활용할지(batch) 또는 실시간으로 처리할지(Kafka)를 결정하기.
    - 사용자가 원하는 데이터의 빈도와 EDA 결과를 고려하여 가장 적합한 데이터 처리 방식을 선택하기.
